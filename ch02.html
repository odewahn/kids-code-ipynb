<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>think stats</title>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>
    <link rel="stylesheet" type="text/css" href="theme/html/html.css"/>
  </head>
  <body data-type="book">
    <section data-type="chapter" id="descriptive" data-pdf-bookmark="Chapter 2. Descriptive Statistics">
<h1>Descriptive Statistics</h1>







<section data-type="sect1" id="mean" data-pdf-bookmark="Means and Averages">
<h1>Means and Averages</h1>

<p>In the previous chapter, I mentioned three summary statistics—mean, variance, and median—without explaining what they are. So before we go any farther, let’s take care of that.</p>

<p>If you have a sample of <em>n</em> values, <em>x</em> <em>i</em>, the mean, <em>μ</em>, is the sum of the values divided by the number of values; in other words</p>

<p>The words “mean” and “average” are sometimes used interchangeably, but I will maintain this distinction:</p>
<ul>
<li>
<p>The “mean” of a sample is the summary statistic computed with the previous formula.</p>
</li>
<li>
<p>An “average” is one of many summary statistics you might choose to describe the typical value or the <em>central tendency</em> of a sample.</p>
</li>
</ul>

<p>Sometimes the mean is a good description of a set of values. For example, apples are all pretty much the same size (at least the ones sold in supermarkets). So if I buy six apples and the total weight is three pounds, it would be reasonable to conclude that they are about a half pound each.</p>

<p>But pumpkins are more diverse. Suppose I grow several varieties in my garden, and one day I harvest three decorative pumpkins that are one pound each, two pie pumpkins that are three pounds each, and one Atlantic Giant pumpkin that weighs 591 pounds. The mean of this sample is 100 pounds, but if I told you “The average pumpkin in my garden is 100 pounds,” that would be wrong, or at least misleading.</p>

<p>In this example, there is no meaningful average because there is no typical pumpkin.</p>
</section>













<section data-type="sect1" id="a0000000020" data-pdf-bookmark="Variance">
<h1>Variance</h1>

<p>If there is no single number that summarizes pumpkin weights, we can do a little better with two numbers: mean and <em>variance</em>.</p>

<p>In the same way that the mean is intended to describe the central tendency, variance is intended to describe the <em>spread</em>. The variance of a set of values is</p>

<p>The term <em>x</em> <em>i</em>-<em>μ</em> is called the “deviation from the mean,” so variance is the mean squared deviation, which is why it is denoted <em>σ__2</em>. The square root of variance, <em>σ</em>, is called the <em>standard deviation</em>.</p>

<p>By itself, variance is hard to interpret. One problem is that the units are strange; in this case, the measurements are in pounds, so the variance is in pounds squared. Standard deviation is more meaningful; in this case, the units are pounds.</p>

<p>.</p>
<div data-type="example">
<h5/>

<p>.</p></div>

<p>If you have prior experience, you might have seen a formula for variance with <em>n</em> − 1 in the denominator, rather than <em>n</em>. This statistic is called the “sample variance,” and it is used to estimate the variance in a population using a sample. We will come back to this in <a data-type="xref" href="ch07.html#estimation">Chapter 7</a>.</p>
</section>













<section data-type="sect1" id="distributions" data-pdf-bookmark="Distributions">
<h1>Distributions</h1>

<p>Summary statistics are concise, but dangerous, because they obscure the data. An alternative is to look at the <em>distribution</em> of the data, which describes how often each value appears.</p>

<p>The most common representation of a distribution is a <em>histogram</em>, which is a graph that shows the frequency or probability of each value.</p>

<p>In this context, <em>frequency</em> means the number of times a value appears in a dataset—it has nothing to do with the pitch of a sound or tuning of a radio signal. A <em>probability</em> is a frequency expressed as a fraction of the sample size, <em>n</em>.</p>

<p>In Python, an efficient way to compute frequencies is with a dictionary. Given a sequence of values, <code>t</code>:</p>


<pre data-type="programlisting">hist = {}
for x in t:
    hist[x] = hist.get(x, 0) + 1</pre>


<p>The result is a dictionary that maps from values to frequencies. To get from frequencies to probabilities, we divide through by <em>n</em>, which is called <em>normalization</em>:</p>


<pre data-type="programlisting">n = float(len(t))
pmf = {}
for x, freq in hist.items():
    pmf[x] = freq / n</pre>


<p>The normalized histogram is called a <em>PMF</em>, which stands for “probability mass function”; that is, it’s a function that maps from values to probabilities (I’ll explain “mass” in <a data-type="xref" href="#expo_pdf">???</a>).</p>

<p>It might be confusing to call a Python dictionary a function. In mathematics, a function is a map from one set of values to another. In Python, we <em>usually</em> represent mathematical functions with function objects, but in this case we are using a dictionary (dictionaries are also called “maps,” if that helps).</p>
</section>













<section data-type="sect1" id="a0000000025" data-pdf-bookmark="Representing Histograms">
<h1>Representing Histograms</h1>

<p>I wrote a Python module called <code>Pmf.py</code> that contains class definitions for Hist objects, which represent histograms, and Pmf objects, which represent PMFs. You can read the documentation at <a href="http://thinkstats.com/Pmf.html">http://thinkstats.com/Pmf.html</a> and download the code from <a href="http://thinkstats.com/Pmf.py">http://thinkstats.com/Pmf.py</a>.</p>

<p>The function <code>MakeHistFromList</code> takes a list of values and returns a new Hist object. You can test it in Python’s interactive mode:</p>


<pre data-type="programlisting">&amp;gt;&amp;gt;&amp;gt; import Pmf
&amp;gt;&amp;gt;&amp;gt; hist = Pmf.MakeHistFromList([1, 2, 2, 3, 5])
&amp;gt;&amp;gt;&amp;gt; print hist
&amp;lt;Pmf.Hist object at 0xb76cf68c&amp;gt;</pre>


<p><code>Pmf.Hist</code> means that this object is a member of the Hist class, which is defined in the Pmf module. In general, I use uppercase letters for the names of classes and functions, and lowercase letters for variables.</p>

<p>Hist objects provide methods to look up values and their probabilities. <code>Freq</code> takes a value and returns its frequency:</p>


<pre data-type="programlisting">&amp;gt;&amp;gt;&amp;gt; hist.Freq(2)
2</pre>


<p>If you look up a value that has never appeared, the frequency is 0.</p>


<pre data-type="programlisting">&amp;gt;&amp;gt;&amp;gt; hist.Freq(4)
0</pre>


<p><code>Values</code> returns an unsorted list of the values in the Hist:</p>


<pre data-type="programlisting">&amp;gt;&amp;gt;&amp;gt; hist.Values()
[1, 5, 3, 2]</pre>


<p>To loop through the values in order, you can use the built-in function <code>sorted</code>:</p>


<pre data-type="programlisting">for val in sorted(hist.Values()):
    print val, hist.Freq(val)</pre>


<p>If you are planning to look up all of the frequencies, it is more efficient to use <code>Items</code>, which returns an unsorted list of value-frequency pairs:</p>


<pre data-type="programlisting">for val, freq in hist.Items():
     print val, freq</pre>


<p>.</p>
<div data-type="example">
<h5/>

<p>=== Plotting Histograms</p>

<p>There are a number of Python packages for making figures and graphs. The one I will demonstrate is <code>pyplot</code>, which is part of the <code>matplotlib</code> package at <a href="http://matplotlib.sourceforge.net">http://matplotlib.sourceforge.net</a>.</p>

<p>This package is included in many Python installations. To see whether you have it, launch the Python interpreter and run:</p>


<pre data-type="programlisting">import matplotlib.pyplot as pyplot
pyplot.pie([1,2,3])
pyplot.show()</pre>


<p>If you have <code>matplotlib</code> you should see a simple pie chart; otherwise, you will have to install it.</p>

<p>Histograms and PMFs are most often plotted as bar charts. The <code>pyplot</code> function to draw a bar chart is <code>bar</code>. Hist objects provide a method called <code>Render</code> that returns a sorted list of values and a list of the corresponding frequencies, which is the format <code>bar</code> expects:</p>


<pre data-type="programlisting">&amp;gt;&amp;gt;&amp;gt; vals, freqs = hist.Render()
&amp;gt;&amp;gt;&amp;gt; rectangles = pyplot.bar(vals, freqs)
&amp;gt;&amp;gt;&amp;gt; pyplot.show()</pre>


<p>I wrote a module called <code>myplot.py</code> that provides functions for plotting histograms, PMFs, and other objects we will see soon. You can read the documentation at <code>thinkstats.com/myplot.html</code> and download the code from <code>thinkstats.com/myplot.py</code>. Or you can use <code>pyplot</code> directly, if you prefer. Either way, you can find the documentation for <code>pyplot</code> on the web.</p>

<p><a data-type="xref" href="#nsfg_hist">Figure 2-1</a> shows histograms of pregnancy lengths for first babies and others.</p>

<figure id="nsfg_hist">
<img src="figs/web/thst_0201.png" alt="thst 0201"/>
<figcaption>Histogram of pregnancy lengths</figcaption>
</figure>

<p>Histograms are useful because they make the following features immediately apparent:</p>
<dl>
<dt>Mode</dt>
<dd>
<p>The most common value in a distribution is called the <em>mode</em>. In <a data-type="xref" href="#nsfg_hist">Figure 2-1</a>, there is a clear mode at 39 weeks. In this case, the mode is the summary statistic that does the best job of describing the typical value.</p>
</dd>
<dt>Shape</dt>
<dd>
<p>Around the mode, the distribution is asymmetric; it drops off quickly to the right and more slowly to the left. From a medical point of view, this makes sense. Babies are often born early, but seldom later than 42 weeks. Also, the right side of the distribution is truncated because doctors often intervene after 42 weeks.</p>
</dd>
<dt>Outliers</dt>
<dd>
<p>Values far from the mode are called <em>outliers</em>. Some of these are just unusual cases, like babies born at 30 weeks. But many of them are probably due to errors, either in the reporting or recording of data.</p>
</dd>
</dl>

<p>Although histograms make some features apparent, they are usually not useful for comparing two distributions. In this example, there are fewer “first babies” than “others,” so some of the apparent differences in the histograms are due to sample sizes. We can address this problem using PMFs.</p>

<p>=== Representing PMFs</p>

<p><code>Pmf.py</code> provides a class called <code>Pmf</code> that represents PMFs. The notation can be confusing, but here it is: <code>Pmf</code> is the name of the module and also the class, so the full name of the class is <code>Pmf.Pmf</code>. I often use <code>pmf</code> as a variable name. Finally, in the text, I use PMF to refer to the general concept of a probability mass function, independent of my implementation.</p>

<p>To create a Pmf object, use <code>MakePmfFromList</code>, which takes a list of values:</p>


<pre data-type="programlisting">&amp;gt;&amp;gt;&amp;gt; import Pmf
&amp;gt;&amp;gt;&amp;gt; pmf = Pmf.MakePmfFromList([1, 2, 2, 3, 5])
&amp;gt;&amp;gt;&amp;gt; print pmf
&amp;lt;Pmf.Pmf object at 0xb76cf68c&amp;gt;</pre>


<p>Pmf and Hist objects are similar in many ways. The methods <code>Values</code> and <code>Items</code> work the same way for both types. The biggest difference is that a Hist maps from values to integer counters; a Pmf maps from values to floating-point probabilities.</p>

<p>To look up the probability associated with a value, use <code>Prob</code>:</p>


<pre data-type="programlisting">&amp;gt;&amp;gt;&amp;gt; pmf.Prob(2)
0.4</pre>


<p>You can modify an existing Pmf by incrementing the probability associated with a value:</p>


<pre data-type="programlisting">&amp;gt;&amp;gt;&amp;gt; pmf.Incr(2, 0.2)
&amp;gt;&amp;gt;&amp;gt; pmf.Prob(2)
0.6</pre>


<p>Or you can multiple a probability by a factor:</p>


<pre data-type="programlisting">&amp;gt;&amp;gt;&amp;gt; pmf.Mult(2, 0.5)
&amp;gt;&amp;gt;&amp;gt; pmf.Prob(2)
0.3</pre>


<p>If you modify a Pmf, the result may not be normalized; that is, the probabilities may no longer add up to 1. To check, you can call <code>Total</code>, which returns the sum of the probabilities:</p>


<pre data-type="programlisting">&amp;gt;&amp;gt;&amp;gt; pmf.Total()
0.9</pre>


<p>To renormalize, call <code>Normalize</code>:</p>


<pre data-type="programlisting">&amp;gt;&amp;gt;&amp;gt; pmf.Normalize()
&amp;gt;&amp;gt;&amp;gt; pmf.Total()
1.0</pre>


<p>Pmf objects provide a <code>Copy</code> method so you can make and and modify a copy without affecting the original.</p>

<p>.</p></div>

<p>.</p>
<div data-type="example">
<h5/>

<p>=== Plotting PMFs</p>

<p>There are two common ways to plot Pmfs:</p>
<ul>
<li>
<p>To plot a Pmf as a bar graph, you can use <code>pyplot.bar</code> or <code>myplot.Hist</code>. Bar graphs are most useful if the number of values in the Pmf is small.</p>
</li>
<li>
<p>To plot a Pmf as a line, you can use <code>pyplot.plot</code> or <code>myplot.Pmf</code>. Line plots are most useful if there are a large number of values and the Pmf is smooth.</p>
</li>
</ul>

<p><a data-type="xref" href="#nsfg_pmf">Figure 2-2</a> shows the PMF of pregnancy lengths as a bar graph. Using the PMF, we can see more clearly where the distributions differ. First babies seem to be less likely to arrive on time (week 39) and more likely to be late (weeks 41 and 42).</p>

<figure id="nsfg_pmf">
<img src="figs/web/thst_0202.png" alt="thst 0202"/>
<figcaption>PMF of pregnancy lengths</figcaption>
</figure>

<p>The code that generates the figures in this chapters is available from <a href="http://thinkstats.com/descriptive.py">http://thinkstats.com/descriptive.py</a>. To run it, you will need the modules it imports and the data from the NSFG (see <a data-type="xref" href="ch01.html#nsfg">“The National Survey of Family Growth”</a>).</p>

<p>Note: <code>pyplot</code> provides a function called <code>hist</code> that takes a sequence of values, computes the histogram, and plots it. Since I use <code>Hist</code> objects, I usually don’t use <code>pyplot.hist</code>.</p>

<p>=== Outliers</p>

<p>Outliers are values that are far from the central tendency. Outliers might be caused by errors in collecting or processing the data, or they might be correct but unusual measurements. It is always a good idea to check for outliers, and sometimes it is useful and appropriate to discard them.</p>

<p>In the list of pregnancy lengths for live births, the ten lowest values are {0, 4, 9, 13, 17, 17, 18, 19, 20, 21}. Values below 20 weeks are certainly errors, and values higher than 30 weeks are probably legitimate. But values in between are hard to interpret.</p>

<p>On the other end, the highest values are:</p>


<pre data-type="programlisting">weeks  count
43     148
44      46
45      10
46       1
47       1
48       7
50       2</pre>


<p>Again, some values are almost certainly errors, but it is hard to know for sure. One option is to <em>trim</em> the data by discarding some fraction of the highest and lowest values (see <a href="http://wikipedia.org/wiki/Truncated_mean">http://wikipedia.org/wiki/Truncated_mean</a>).</p>

<p>=== Other Visualizations</p>

<p>Histograms and PMFs are useful for exploratory data analysis; once you have an idea of what is going on, it is often useful to design a visualization that focuses on the apparent effect.</p>

<p>In the NSFG data, the biggest differences in the distributions are near the mode. So it makes sense to zoom in on that part of the graph, and to transform the data to emphasize differences.</p>

<p><a data-type="xref" href="#nsfg_diffs">Figure 2-3</a> shows the difference between the PMFs for weeks 35–45. I multiplied by 100 to express the differences in percentage points.</p>

<figure id="nsfg_diffs">
<img src="figs/web/thst_0203.png" alt="thst 0203"/>
<figcaption>Difference in percentage, by week</figcaption>
</figure>

<p>This figure makes the pattern clearer: first babies are less likely to be born in week 39, and somewhat more likely to be born in weeks 41 and 42.</p>

<p>=== Relative Risk</p>

<p>We started with the question, “Do first babies arrive late?” To make that more precise, let’s say that a baby is early if it is born during Week 37 or earlier, on time if it is born during Week 38, 39, or 40, and late if it is born during Week 41 or later. Ranges like these that are used to group data are called <em>bins</em>.</p>

<p>.</p></div>
</section>













<section data-type="sect1" id="a0000000041" data-pdf-bookmark="Conditional Probability">
<h1>Conditional Probability</h1>

<p>Imagine that someone you know is pregnant, and it is the beginning of Week 39. What is the chance that the baby will be born in the next week? How much does the answer change if it’s a first baby?</p>

<p>We can answer these questions by computing a <em>conditional probability</em>, which is (ahem!) a probability that depends on a condition. In this case, the condition is that we know the baby didn’t arrive during Weeks 0–38.</p>

<p>Here’s one way to do it:</p>
<ol>
<li>
<p>Given a PMF, generate a fake cohort of 1,000 pregnancies. For each number of weeks, <em>x</em>, the number of pregnancies with duration <em>x</em> is 1,000 PMF(<em>x</em>).</p>
</li>
<li>
<p>Remove from the cohort all pregnancies with length less than 39.</p>
</li>
<li>
<p>Compute the PMF of the remaining durations; the result is the conditional PMF.</p>
</li>
<li>
<p>Evaluate the conditional PMF at <em>x</em> = 39 weeks.</p>
</li>

</ol>

<p>This algorithm is conceptually clear, but not very efficient. A simple alternative is to remove from the distribution the values less than 39 and then renormalize.</p>

<p>.</p>
<div data-type="example">
<h5/>

<p>=== Reporting Results</p>

<p>At this point, we have explored the data and seen several apparent effects. For now, let’s assume that these effects are real (but let’s remember that it’s an assumption). How should we report these results?</p>

<p>The answer might depend on who is asking the question. For example, a scientist might be interested in any (real) effect, no matter how small. A doctor might only care about effects that are <em>clinically significant</em>; that is, differences that affect treatment decisions. A pregnant woman might be interested in results that are relevant to her, like the conditional probabilities in the previous section.</p>

<p>How you report results also depends on your goals. If you are trying to demonstrate the significance of an effect, you might choose summary statistics, like relative risk, that emphasize differences. If you are trying to reassure a patient, you might choose statistics that put the differences in context.</p>

<p>.</p></div>
</section>













<section data-type="sect1" id="a0000000045" data-pdf-bookmark="Glossary">
<h1>Glossary</h1>
<dl>
<dt>bin</dt>
<dd>
<p>A range used to group nearby values.</p>
</dd>
<dt>central tendency</dt>
<dd>
<p>A characteristic of a sample or population; intuitively, it is the most average value.</p>
</dd>
<dt>clinically significant</dt>
<dd>
<p>A result, like a difference between groups, that is relevant in practice.</p>
</dd>
<dt>conditional probability</dt>
<dd>
<p>A probability computed under the assumption that some condition holds.</p>
</dd>
<dt>distribution</dt>
<dd>
<p>A summary of the values that appear in a sample and the frequency, or probability, of each.</p>
</dd>
<dt>frequency</dt>
<dd>
<p>The number of times a value appears in a sample.</p>
</dd>
<dt>histogram</dt>
<dd>
<p>A mapping from values to frequencies, or a graph that shows this mapping.</p>
</dd>
<dt>mode</dt>
<dd>
<p>The most frequent value in a sample.</p>
</dd>
<dt>normalization</dt>
<dd>
<p>The process of dividing a frequency by a sample size to get a probability.</p>
</dd>
<dt>outlier</dt>
<dd>
<p>A value far from the central tendency.</p>
</dd>
<dt>probability</dt>
<dd>
<p>A frequency expressed as a fraction of the sample size.</p>
</dd>
<dt>probability mass function (PMF)</dt>
<dd>
<p>A representation of a distribution as a function that maps from values to probabilities.</p>
</dd>
<dt>relative risk</dt>
<dd>
<p>A ratio of two probabilities, often used to measure a difference between distributions.</p>
</dd>
<dt>spread</dt>
<dd>
<p>A characteristic of a sample or population; intuitively, it describes how much variability there is.</p>
</dd>
<dt>standard deviation</dt>
<dd>
<p>The square root of variance, also used as a measure of spread.</p>
</dd>
<dt>trim</dt>
<dd>
<p>To remove outliers from a dataset.</p>
</dd>
<dt>variance</dt>
<dd>
<p>A summary statistic often used to quantify spread.</p>
</dd>
</dl>
</section>







</section>
  </body>
</html>
