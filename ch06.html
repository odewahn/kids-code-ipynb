<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>think stats</title>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>
    <link rel="stylesheet" type="text/css" href="theme/html/html.css"/>
  </head>
  <body data-type="book">
    <section data-type="chapter" id="operations" data-pdf-bookmark="Chapter 6. Operations on Distributions">
<h1>Operations on Distributions</h1>







<section data-type="sect1" id="a0000000145" data-pdf-bookmark="Skewness">
<h1>Skewness</h1>

<p><em>Skewness</em> is a statistic that measures the asymmetry of a distribution. Given a sequence of values, <em>x</em> <em>i</em>, the sample skewness is:</p>

<p>You might recognize <em>m<em>2</em> as the mean squared deviation (also known as variance); <em>m</em>3</em> is the mean cubed deviation.</p>

<p>Negative skewness indicates that a distribution “skews left”; that is, it extends farther to the left than the right. Positive skewness indicates that a distribution skews right.</p>

<p>In practice, computing the skewness of a sample is usually not a good idea. If there are any outliers, they have a disproportionate effect on <em>g__1</em>.</p>

<p>Another way to evaluate the asymmetry of a distribution is to look at the relationship between the mean and median. Extreme values have more effect on the mean than the median, so in a distribution that skews left, the mean is less than the median.</p>

<p><em>Pearson’s median skewness coefficient</em> is an alternative measure of skewness that explicitly captures the relationship between the mean, <em>μ</em>, and the median, <em>μ__1/2</em>:</p>

<p>This statistic is <em>robust</em>, which means that it is less vulnerable to the effect of outliers.</p>

<p>.</p>
<div data-type="example">
<h5/>

<p>.</p></div>

<p>.</p>
<div data-type="example">
<h5/>

<p>=== Random Variables</p>

<p>A <em>random variable</em> represents a process that generates a random number. Random variables are usually written with a capital letter, like <em>X</em>. When you see a random variable, you should think “a value selected from a distribution.”</p>

<p>For example, the formal definition of the cumulative distribution function is:</p>

<p>CDF_X_(<em>x</em>)
=
<em>P</em>(<em>X</em> ≤ <em>x</em>)</p>

<p>I have avoided this notation until now because it is so awful, but here’s what it means: the CDF of the random variable <em>X</em>, evaluated for a particular value <em>x</em>, is defined as the probability that a value generated by the random process <em>X</em> is less than or equal to <em>x</em>.</p>

<p>As a computer scientist, I find it helpful to think of a random variable as an object that provides a method, which I will call <code>generate</code>, that uses a random process to generate values.</p>

<p>For example, here is a definition for a class that represents random variables:</p>


<pre data-type="programlisting">class RandomVariable(object):
    """Parent class for all random variables."""</pre>


<p>And here is a random variable with an exponential distribution:</p>


<pre data-type="programlisting">class Exponential(RandomVariable):
    def __init__(self, lam):
        self.lam = lam

    def generate(self):
        return random.expovariate(self.lam)</pre>


<p>The init method takes the parameter, <em>λ</em>, and stores it as an attribute. The <code>generate</code> method returns a random value from the exponential distribution with that parameter.</p>

<p>Each time you invoke <code>generate</code>, you get a different value. The value you get is called a <em>random variate</em>, which is why many function names in the <code>random</code> module include the word “variate.”</p>

<p>If I were just generating exponential variates, I would not bother to define a new class; I would use <code>random.expovariate</code>. But for other distributions, it might be useful to use RandomVariable objects. For example, the Erlang distribution is a continuous distribution with parameters <em>λ</em> and <em>k</em> (see <a href="http://wikipedia.org/wiki/Erlang_distribution">http://wikipedia.org/wiki/Erlang_distribution</a>).</p>

<p>One way to generate values from an Erlang distribution is to add <em>k</em> values from an exponential distribution with the same <em>λ</em>. Here’s an implementation:</p>


<pre data-type="programlisting">class Erlang(RandomVariable):
    def __init__(self, lam, k):
        self.lam = lam
        self.k = k
        self.expo = Exponential(lam)

    def generate(self):
        total = 0
        for i in range(self.k):
            total += self.expo.generate()
        return total</pre>


<p>The init method creates an Exponential object with the given parameter; then <code>generate</code> uses it. In general, the init method can take any set of parameters and the <code>generate</code> function can implement any random process.</p>

<p>.</p></div>
</section>













<section data-type="sect1" id="density" data-pdf-bookmark="PDFs">
<h1>PDFs</h1>

<p>The derivative of a CDF is called a <em>probability density function</em>, or <em>PDF</em>. For example, the PDF of an exponential distribution is</p>

<p>The PDF of a normal distribution is</p>

<p>Evaluating a PDF for a particular value of <em>x</em> is usually not useful. The result is not a probability; it is a probability <em>density</em>.</p>

<p>In physics, density is mass per unit of volume; in order to get a mass, you have to multiply by volume or, if the density is not constant, you have to integrate over volume.</p>

<p>Similarly, probability density measures probability per unit of <em>x</em>. In order to get a probability mass,<a data-type="noteref" id="idp1510000-marker" href="ch06.html#idp1510000"><sup>14</sup></a> you have to integrate over <em>x</em>. For example, if <em>x</em> is a random variable whose PDF is PDF_X_, we can compute the probability that a value from <em>X</em> falls between −0.5 and 0.5:</p>

<p>Or, since the CDF is the integral of the PDF, we can write</p>

<p>For some distributions, we can evaluate the CDF explicitly, so we would use the second option. Otherwise, we usually have to integrate the PDF numerically.</p>

<p>.</p>
<div data-type="example">
<h5/>

<p>.</p></div>
</section>













<section data-type="sect1" id="a0000000162" data-pdf-bookmark="Convolution">
<h1>Convolution</h1>

<p>Suppose we have two random variables, <em>X</em> and <em>Y</em>, with distributions CDF_X_ and CDF_Y_. What is the distribution of the sum <em>Z</em> = <em>X</em> + <em>Y</em>?</p>

<p>One option is to write a RandomVariable object that generates the sum:</p>


<pre data-type="programlisting">class Sum(RandomVariable):
    def __init__(X, Y):
        self.X = X
        self.Y = Y

    def generate():
        return X.generate() + Y.generate()</pre>


<p>Given any RandomVariables, <em>X</em> and <em>Y</em>, we can create a Sum object that represents <em>Z</em>. Then we can use a sample from <em>Z</em> to approximate CDF_Z_.</p>

<p>This approach is simple and versatile, but not very efficient; we have to generate a large sample to estimate CDF_Z_ accurately, and even then it is not exact.</p>

<p>If CDF_X_ and CDF_Y_ are expressed as functions, sometimes we can find CDF_Z_ exactly. Here’s how:</p>
<ol>
<li>
<p>To start, assume that the particular value of <em>X</em> is <em>x</em>. Then CDF_Z_(<em>z</em>) is</p>
</li>

</ol>

<p>Let’s read that back. The left side is “the probability that the sum is less than <em>z</em>, given that the first term is <em>x</em>.” Well, if the first term is <em>x</em> and the sum has to be less than <em>z</em>, then the second term has to be less than <em>z</em> − <em>x</em>.</p>
<ol>
<li>
<p>To get the probability that <em>Y</em> is less than <em>z</em> − <em>x</em>, we evaluate CDF_Y_.</p>
</li>

</ol>

<p>This follows from the definition of the CDF.</p>
<ol>
<li>
<p>Good so far? Let’s go on. Since we don’t actually know the value of <em>x</em>, we have to consider all values it could have and integrate over them:</p>
</li>

</ol>

<p>The integrand is “the probability that <em>Z</em> is less than or equal to <em>z</em>, given that <em>X</em> = <em>x</em>, times the probability that <em>X</em> = <em>x</em>.”</p>

<p>Substituting from the previous steps we get</p>

<p>The left side is the definition of CDF_Z_, so we conclude:</p>
<ol>
<li>
<p>To get PDF_Z_, take the derivative of both sides with respect to <em>z</em>. The result is</p>
</li>

</ol>

<p>If you have studied signals and systems, you might recognize that integral. It is the <strong>convolution</strong> of PDF_Y_ and PDF_X_, denoted with the operator ∗.</p>

<p>PDF_Z_=
PDF_Y_∗ PDF_X_</p>

<p>So the distribution of the sum is the convolution of the distributions. See <a href="http://wiktionary.org/wiki/booyah">http://wiktionary.org/wiki/booyah</a>!</p>

<p>As an example, suppose <em>X</em> and <em>Y</em> are random variables with an exponential distribution with parameter <em>λ</em>. The distribution of <em>Z</em> = <em>X</em> + <em>Y</em> is:</p>

<p>Now we have to remember that PDFexpo is 0 for all negative values, but we can handle that by adjusting the limits of integration:</p>

<p>Now we can combine terms and move constants outside the integral:</p>

<p>This, it turns out, is the PDF of an Erlang distribution with parameter <em>k</em> = 2 (see <a href="http://wikipedia.org/wiki/Erlang_distribution">http://wikipedia.org/wiki/Erlang_distribution</a>). So the convolution of two exponential distributions (with the same parameter) is an Erlang distribution.</p>

<p>.</p>
<div data-type="example">
<h5/>

<p>.</p></div>

<p>.</p>
<div data-type="example">
<h5/>


<pre data-type="programlisting">for x in pmf_x.Values():
    for y in pmf_y.Values():
        z = x + y</pre>
</div>
</section>













<section data-type="sect1" id="why_normal" data-pdf-bookmark="Why Normal?">
<h1>Why Normal?</h1>

<p>I said earlier that normal distributions are amenable to analysis, but I didn’t say why. One reason is that they are closed under linear transformation and convolution. To explain what that means, it will help to introduce some notation.</p>

<p>If the distribution of a random variable, <em>X</em>, is normal with parameters <em>μ</em> and <em>σ</em>, you can write</p>

<p><em>X</em> ∼ <img src="figs/web/equations/img-0040.png" alt="img-0040"/>(<em>μ</em>,
<em>σ</em>)</p>

<p>where the symbol ∼ means “is distributed” and the script letter <img src="figs/web/equations/img-0040.png" alt="img-0040"/> stands for “normal.”</p>

<p>A linear transformation of <em>X</em> is something like <em>X</em>’ = <em>a</em> <em>X</em> + <em>b</em>, where <em>a</em> and <em>b</em> are real numbers.  A family of distributions is closed under linear transformation if <em>X</em>’ is in the same family as <em>X</em>. The normal distribution has this property; if <em>X</em> ∼ <img src="figs/web/equations/img-0040.png" alt="img-0040"/>(<em>μ</em>, <em>σ__2</em>),</p>

<p><em>X</em>’ ∼ <img src="figs/web/equations/img-0040.png" alt="img-0040"/>(<em>a<em>μ</em> + <em>b</em>,
<em>a</em>2<em>σ</em>2</em>)</p>

<p>Normal distributions are also closed under convolution. If <em>Z</em> = <em>X</em> + <em>Y</em> and <em>X</em> ∼ <img src="figs/web/equations/img-0040.png" alt="img-0040"/>(<em>μ</em> <em>X</em>, <em>σ</em> <em>X<em>2</em>) and <em>Y</em> ∼ <img src="figs/web/equations/img-0040.png" alt="img-0040"/>(<em>μ</em> <em>Y</em>, <em>σ</em> <em>Y</em>2</em>) then</p>

<p>The other distributions we have looked at do not have these properties.</p>

<p>.</p>
<div data-type="example">
<h5/>

<p>.</p></div>
</section>













<section data-type="sect1" id="CLT" data-pdf-bookmark="Central Limit Theorem">
<h1>Central Limit Theorem</h1>

<p>So far we have seen:</p>
<ul>
<li>
<p>If we add values drawn from normal distributions, the distribution of the sum is normal.</p>
</li>
<li>
<p>If we add values drawn from other distributions, the sum does not generally have one of the continuous distributions we have seen.</p>
</li>
</ul>

<p>But it turns out that if we add up a large number of values from almost any distribution, the distribution of the sum converges to normal.</p>

<p>More specifically, if the distribution of the values has mean and standard deviation <em>μ</em> and <em>σ</em>, the distribution of the sum is approximately <img src="figs/web/equations/img-0040.png" alt="img-0040"/>(<em>n<em>μ</em>, <em>n</em>σ__2</em>).</p>

<p>This is called the <em>Central Limit Theorem</em>. It is one of the most useful tools for statistical analysis, but it comes with caveats:</p>
<ul>
<li>
<p>The values have to be drawn independently.</p>
</li>
<li>
<p>The values have to come from the same distribution (although this requirement can be relaxed).</p>
</li>
<li>
<p>The values have to be drawn from a distribution with finite mean and variance, so most Pareto distributions are out.</p>
</li>
<li>
<p>The number of values you need before you see convergence depends on the skewness of the distribution. Sums from an exponential distribution converge for small sample sizes. Sums from a lognormal distribution do not.</p>
</li>
</ul>

<p>The Central Limit Theorem explains, at least in part, the prevalence of normal distributions in the natural world. Most characteristics of animals and other life forms are affected by a large number of genetic and environmental factors whose effect is additive. The characteristics we measure are the sum of a large number of small effects, so their distribution tends to be normal.</p>

<p>.</p>
<div data-type="example">
<h5/>

<p>.</p></div>

<p>.</p>
<div data-type="example">
<h5/>

<p>=== The Distribution Framework</p>

<p>At this point, we have seen PMFs, CDFs, and PDFs; let’s take a minute to review. <a data-type="xref" href="#dist_framework">Figure 6-1</a> shows how these functions relate to each other.</p>

<figure id="dist_framework">
<img src="figs/web/thst_0601.png" alt="thst 0601"/>
<figcaption>A framework that relates representations of distribution functions</figcaption>
</figure>

<p>We started with PMFs, which represent the probabilities for a discrete set of values. To get from a PMF to a CDF, we computed a cumulative sum. To be more consistent, a discrete CDF should be called a cumulative mass function (CMF), but as far as I can tell no one uses that term.</p>

<p>To get from a CDF to a PMF, you can compute differences in cumulative probabilities.</p>

<p>Similarly, a PDF is the derivative of a continuous CDF; or, equivalently, a CDF is the integral of a PDF. But remember that a PDF maps from values to probability densities; to get a probability, you have to integrate.</p>

<p>To get from a discrete to a continuous distribution, you can perform various kinds of smoothing. One form of smoothing is to assume that the data come from an analytic continuous distribution (like exponential or normal) and to estimate the parameters of that distribution. And that’s what <a data-type="xref" href="ch07.html#estimation">Chapter 7</a> is about.</p>

<p>If you divide a PDF into a set of bins, you can generate a PMF that is at least an approximation of the PDF. We use this technique in <a data-type="xref" href="ch07.html#estimation">Chapter 7</a> to do Bayesian estimation.</p>

<p>.</p></div>
</section>













<section data-type="sect1" id="a0000000187" data-pdf-bookmark="Glossary">
<h1>Glossary</h1>
<dl>
<dt>Central Limit Theorem</dt>
<dd>
<p>“The supreme law of Unreason,” according to Sir Francis Galton, an early statistician.</p>
</dd>
<dt>convolution</dt>
<dd>
<p>An operation that computes the distribution of the sum of values from two distributions.</p>
</dd>
<dt>illusory superiority</dt>
<dd>
<p>The tendency of people to imagine that they are better than average.</p>
</dd>
<dt>probability density function (PDF)</dt>
<dd>
<p>The derivative of a continuous CDF.</p>
</dd>
<dt>random variable</dt>
<dd>
<p>An object that represents a random process.</p>
</dd>
<dt>random variate</dt>
<dd>
<p>A value generated by a random process.</p>
</dd>
<dt>robust</dt>
<dd>
<p>A statistic is robust if it is relatively immune to the effect of outliers.</p>
</dd>
<dt>skewness</dt>
<dd>
<p>A characteristic of a distribution; intuitively, it is a measure of how asymmetric the distribution is.</p>
</dd>
</dl>
</section>







<aside data-type="footnotes"><p data-type="footnote" id="idp1510000"><a href="ch06.html#idp1510000-marker"><sup>14</sup></a> To take the analogy one step farther, the mean of a distribution is its center of mass, and the variance is its moment of inertia.</p></aside></section>
  </body>
</html>
