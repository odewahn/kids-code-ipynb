<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>think stats</title>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>
    <link rel="stylesheet" type="text/css" href="theme/html/html.css"/>
  </head>
  <body data-type="book">
    <section data-type="chapter" id="a0000000245" data-pdf-bookmark="Chapter 8. Correlation">
<h1>Correlation</h1>







<section data-type="sect1" id="a0000000246" data-pdf-bookmark="Standard Scores">
<h1>Standard Scores</h1>

<p>In this chapter, we look at relationships between variables. For example, we have a sense that height is related to weight; people who are taller tend to be heavier. <em>Correlation</em> is a description of this kind of relationship.</p>

<p>A challenge in measuring correlation is that the variables we want to compare might not be expressed in the same units. For example, height might be in centimeters and weight in kilograms. And even if they are in the same units, they come from different distributions.</p>

<p>There are two common solutions to these problems:</p>
<ol>
<li>
<p>Transform all values to <em>standard scores</em>. This leads to the Pearson coefficient of correlation.</p>
</li>
<li>
<p>Transform all values to their percentile ranks. This leads to the Spearman coefficient.</p>
</li>

</ol>

<p>If <em>X</em> is a series of values, <em>x</em> <em>i</em>, we can convert to standard scores by subtracting the mean and dividing by the standard deviation: z_i_ = (x_i_ − <em>μ</em>) / <em>σ</em>.</p>

<p>The numerator is a deviation: the distance from the mean. Dividing by <em>σ</em> <em>normalizes</em> the deviation, so the values of <em>Z</em> are dimensionless (no units) and their distribution has mean 0 and variance 1.</p>

<p>If <em>X</em> is normally distributed, so is <em>Z</em>; but if <em>X</em> is skewed or has outliers, so does <em>Z</em>. In those cases, it is more robust to use percentile ranks. If <em>R</em> contains the percentile ranks of the values in <em>X</em>, the distribution of <em>R</em> is uniform between 0 and 100, regardless of the distribution of <em>X</em>.</p>
</section>













<section data-type="sect1" id="a0000000247" data-pdf-bookmark="Covariance">
<h1>Covariance</h1>

<p><em>Covariance</em> is a measure of the tendency of two variables to vary together. If we have two series, <em>X</em> and <em>Y</em>, their deviations from the mean are</p>

<p><em>d</em> <em>x</em> <em>i</em>=
<em>x</em> <em>i</em> − <em>μ</em> <em>X</em></p>

<p><em>d</em> <em>y</em> <em>i</em>=
<em>y</em> <em>i</em> − <em>μ</em> <em>Y</em></p>

<p>where <em>μ</em> <em>X</em> is the mean of <em>X</em> and <em>μ</em> <em>Y</em> is the mean of <em>Y</em>. If <em>X</em> and <em>Y</em> vary together, their deviations tend to have the same sign.</p>

<p>If we multiply them together, the product is positive when the deviations have the same sign and negative when they have the opposite sign. So adding up the products gives a measure of the tendency to vary together.</p>

<p>Covariance is the mean of these products:</p>

<p>where <em>n</em> is the length of the two series (they have to be the same length).</p>

<p>Covariance is useful in some computations, but it is seldom reported as a summary statistic because it is hard to interpret. Among other problems, its units are the product of the units of <em>X</em> and <em>Y</em>. So the covariance of weight and height might be in units of kilogram-meters, which doesn’t mean much.</p>

<p>.</p>
<div data-type="example">
<h5/>

<p>=== Correlation</p>

<p>One solution to this problem is to divide the deviations by <em>σ</em>, which yields standard scores, and compute the product of standard scores:</p>

<p>The mean of these products is</p>

<p>This value is called <em>Pearson’s correlation</em> after Karl Pearson, an influential early statistician. It is easy to compute and easy to interpret. Because standard scores are dimensionless, so is <em>ρ</em>.</p>

<p>Also, the result is necessarily between −1 and +1. To see why, we can rewrite <em>ρ</em> by factoring out <em>σ</em> <em>X</em> and <em>σ</em> <em>Y</em>:</p>

<p>Expressed in terms of deviations, we have</p>

<p>Then, by the ever-useful Cauchy-Schwarz inequality,<a data-type="noteref" id="idp1878768-marker" href="ch08.html#idp1878768"><sup>16</sup></a> we can show that <em>ρ__2</em> ≤ 1, so −1 ≤ <em>ρ</em> ≤ 1.</p>

<p>The magnitude of <em>ρ</em> indicates the strength of the correlation. If <em>ρ</em> = 1, the variables are perfectly correlated, which means that if you know one, you can make a perfect prediction about the other. The same is also true if <em>ρ</em> = −1. It means that the variables are negatively correlated, but for purposes of prediction, a negative correlation is just as good as a positive one.</p>

<p>Most correlation in the real world is not perfect, but it is still useful. For example, if you know someone’s height, you might be able to guess their weight. You might not get it exactly right, but your guess will be better than if you didn’t know the height. Pearson’s correlation is a measure of how much better.</p>

<p>So if <em>ρ</em> = 0, does that mean there is no relationship between the variables? Unfortunately, no. Pearson’s correlation only measures <em>linear</em> relationships. If there’s a nonlinear relationship, <em>ρ</em> understates the strength of the dependence.</p>

<p><a data-type="xref" href="#corr_examples">Figure 8-1</a> is from <a href="http://wikipedia.org/wiki/Correlation_and_dependence">http://wikipedia.org/wiki/Correlation_and_dependence</a>. It shows scatterplots and correlation coefficients for several carefully-constructed datasets.</p>

<figure id="corr_examples">
<img src="figs/web/thst_0901.png" alt="thst 0901"/>
<figcaption>Examples of datasets with a range of correlations</figcaption>
</figure>

<p>The top row shows linear relationships with a range of correlations; you can use this row to get a sense of what different values of <em>ρ</em> look like. The second row shows perfect correlations with a range of slopes, which demonstrates that correlation is unrelated to slope (we’ll talk about estimating slope soon). The third row shows variables that are clearly related, but because the relationship is non-linear, the correlation coefficient is 0.</p>

<p>The moral of this story is that you should always look at a scatterplot of your data before blindly computing a correlation coefficient.</p>

<p>.</p></div>
</section>













<section data-type="sect1" id="a0000000257" data-pdf-bookmark="Making Scatterplots in Pyplot">
<h1>Making Scatterplots in Pyplot</h1>

<p>The simplest way to check for a relationship between two variables is a scatterplot, but making a good scatterplot is not always easy. As an example, I’ll plot weight versus height for the respondents in the BRFSS (see <a data-type="xref" href="#lognormal">???</a>). <code>pyplot</code> provides a function named <code>scatter</code> that makes scatterplots:</p>


<pre data-type="programlisting">import matplotlib.pyplot as pyplot
pyplot.scatter(heights, weights)</pre>


<p><a data-type="xref" href="#scatterplot1">Figure 8-2</a> shows the result. Not surprisingly, it looks like there is a positive correlation: taller people tend to be heavier. But this is not the best representation of the data, because the data is packed into columns. The problem is that the heights were rounded to the nearest inch, converted to centimeters, and then rounded again. Some information is lost in translation.</p>

<figure id="scatterplot1">
<img src="figs/web/thst_0902.png" alt="thst 0902"/>
<figcaption>Simple scatterplot of weight versus height for the respondents in the BRFSS</figcaption>
</figure>

<p>We can’t get that information back, but we can minimize the effect on the scatterplot by <em>jittering</em> the data, which means adding random noise to reverse the effect of rounding off. Since these measurements were rounded to the nearest inch, they can be off by up to 0.5 inches or 1.3 cm. So I added uniform noise in the range −1.3 to 1.3:</p>


<pre data-type="programlisting">jitter = 1.3
heights = [h + random.uniform(-jitter, jitter) for h in heights]</pre>


<p><a data-type="xref" href="#scatterplot2">Figure 8-3</a> shows the result. Jittering the data makes the shape of the relationship clearer. In general, you should only jitter data for purposes of visualization and avoid using jittered data for analysis.</p>

<figure id="scatterplot2">
<img src="figs/web/thst_0903.png" alt="thst 0903"/>
<figcaption>Scatterplot with jittered data</figcaption>
</figure>

<p>Even with jittering, this is not the best way to represent the data. There are many overlapping points, which hides data in the dense parts of the figure and gives disproportionate emphasis to outliers.</p>

<p>We can solve that with the <code>alpha</code> parameter, which makes the points partly transparent:</p>


<pre data-type="programlisting">pyplot.scatter(heights, weights, alpha=0.2)</pre>


<p><a data-type="xref" href="#scatterplot3">Figure 8-4</a> shows the result. Overlapping data points look darker, so darkness is proportional to density. In this version of the plot, we can see an apparent artifact: a horizontal line near 90 kg or 200 pounds. Since this data is based on self-reports in pounds, the most likely explanation is some responses were rounded off (possibly down).</p>

<figure id="scatterplot3">
<img src="figs/web/thst_0904.png" alt="thst 0904"/>
<figcaption>Scatterplot with jittering and transparency</figcaption>
</figure>

<p>Using transparency works well for moderate-sized datasets, but this figure only shows the first 1,000 records in the BRFSS, out of a total of 414,509.</p>

<p>To handle larger datasets, one option is a hexbin plot, which divides the graph into hexagonal bins and colors each bin according to how many data points fall in it. <code>pyplot</code> provides a function called <code>hexbin</code>:</p>


<pre data-type="programlisting">pyplot.hexbin(heights, weights, cmap=matplotlib.cm.Blues)</pre>


<p><a data-type="xref" href="#scatterplot4">Figure 8-5</a> shows the result with a blue colormap. An advantage of a hexbin is that it shows the shape of the relationship well, and it is efficient for large datasets. A drawback is that it makes the outliers invisible.</p>

<figure id="scatterplot4">
<img src="figs/web/thst_0905.png" alt="thst 0905"/>
<figcaption>Scatterplot with binned data using pyplot.hexbin</figcaption>
</figure>

<p>The moral of this story is that it is not easy to make a scatterplot that is not potentially misleading. You can download the code for these figures from <a href="http://thinkstats.com/brfss_scatter.py">http://thinkstats.com/brfss_scatter.py</a>.</p>
</section>













<section data-type="sect1" id="a0000000262" data-pdf-bookmark="Spearman’s Rank Correlation">
<h1>Spearman’s Rank Correlation</h1>

<p>Pearson’s correlation works well if the relationship between variables is linear and if the variables are roughly normal. But it is not robust in the presence of outliers.</p>

<p>Anscombe’s quartet demonstrates this effect; it contains four datasets with the same correlation. One is a linear relation with random noise, one is a non-linear relation, one is a perfect relation with an outlier, and one has no relation except an artifact caused by an outlier. You can read more about it at <a href="http://wikipedia.org/wiki/Anscombe’s_quartet">http://wikipedia.org/wiki/Anscombe’s_quartet</a>.</p>

<p>Spearman’s rank correlation is an alternative that mitigates the effect of outliers and skewed distributions. To compute Spearman’s correction, we have to compute the <em>rank</em> of each value, which is its index in the sorted sample. For example, in the sample {7, 1, 2, 5} the rank of the value 5 is 3, because it appears third if we sort the elements. Then we compute Pearson’s correlation for the ranks.</p>

<p>An alternative to Spearman’s is to apply a transform that makes the data more nearly normal, the compute Pearson’s correlation for the transformed data. For example, if the data is approximately lognormal, you could take the log of each value and compute the correlation of the logs.</p>

<p>.</p>
<div data-type="example">
<h5/>

<p>.</p></div>
</section>













<section data-type="sect1" id="a0000000265" data-pdf-bookmark="Least Squares Fit">
<h1>Least Squares Fit</h1>

<p>Correlation coefficients measure the strength and sign of a relationship, but not the slope. There are several ways to estimate the slope; the most common is a <em>linear least squares fit</em>. A “linear fit” is a line intended to model the relationship between variables. A “least squares” fit is one that minimizes the mean squared error (MSE) between the line and the data.<a data-type="noteref" id="idp1927616-marker" href="ch08.html#idp1927616"><sup>17</sup></a></p>

<p>Suppose we have a sequence of points, <em>Y</em>, that we want to express as a function of another sequence <em>X</em>. If there is a linear relationship between <em>X</em> and <em>Y</em> with intercept <em>α</em> and slope <em>β</em>, we expect each <em>y</em> <em>i</em> to be roughly <em>α</em> + <em>β</em> <em>x</em> <em>i</em>.</p>

<p>But unless the correlation is perfect, this prediction is only approximate. The deviation, or <em>residual</em>, is</p>

<p><em>ε</em> <em>i</em> = (<em>α</em> + <em>β</em> <em>x</em> <em>i</em>) − <em>y</em> <em>i</em></p>

<p>The residual might be due to random factors like measurement error, or non-random factors that are unknown. For example, if we are trying to predict weight as a function of height, unknown factors might include diet, exercise, and body type.</p>

<p>If we get the parameters <em>α</em> and <em>β</em> wrong, the residuals get bigger, so it makes intuitive sense that the parameters we want are the ones that minimize the residuals.</p>

<p>As usual, we could minimize the absolute value of the residuals, or their squares, or their cubes, etc. The most common choice is to minimize the sum of squared residuals.</p>

<p>Why? There are three good reasons and one bad one:</p>
<ul>
<li>
<p>Squaring has the obvious feature of treating positive and negative residuals the same, which is usually what we want.</p>
</li>
<li>
<p>Squaring gives more weight to large residuals, but not so much weight that the largest residual always dominates.</p>
</li>
<li>
<p>If the residuals are independent of <em>x</em>, random, and normally distributed with <em>μ</em> = 0 and constant (but unknown) <em>σ</em>, then the least squares fit is also the maximum likelihood estimator of <em>α</em> and <em>β</em>.<a data-type="noteref" id="idp1949712-marker" href="ch08.html#idp1949712"><sup>18</sup></a></p>
</li>
<li>
<p>The values of <img src="figs/web/equations/img-0075.png" alt="img-0075"/> and <img src="figs/web/equations/img-0076.png" alt="img-0076"/> that minimize the squared residuals can be computed efficiently.</p>
</li>
</ul>

<p>That last reason made sense when computational efficiency was more important than choosing the method most appropriate to the problem at hand. That’s no longer the case, so it is worth considering whether squared residuals are the right thing to minimize.</p>

<p>For example, if you are using values of <em>X</em> to predict values of <em>Y</em>, guessing too high might be better (or worse) than guessing too low. In that case, you might want to compute some cost function, cost(<em>ε</em> <em>i</em>), and minimize total cost.</p>

<p>However, computing a least squares fit is quick, easy, and often good enough, so here’s how:</p>
<ol>
<li>
<p>Compute the sample means, x̄ and ȳ, the variance of <em>X</em>, and the covariance of <em>X</em> and <em>Y</em>.</p>
</li>
<li>
<p>The estimated slope is</p>
</li>
<li>
<p>And the intercept is</p>
</li>

</ol>

<p>To see how this is derived, you can read <a href="http://wikipedia.org/wiki/Numerical_methods_for_linear_least_squares">http://wikipedia.org/wiki/Numerical_methods_for_linear_least_squares</a>.</p>

<p>.</p>
<div data-type="example">
<h5/>

<p>.</p></div>

<p>.</p>
<div data-type="example">
<h5/>

<p>=== Goodness of Fit</p>

<p>Having fit a linear model to the data, we might want to know how good it is. Well, that depends on what it’s for. One way to evaluate a model is its predictive power.</p>

<p>In the context of prediction, the quantity we are trying to guess is called a <em>dependent variable</em> and the quantity we are using to make the guess is called an <em>explanatory</em> or <em>independent variable</em>.</p>

<p>To measure the predictive power of a model, we can compute the <em>coefficient of</em> <em>determination</em>, more commonly known as “R-squared”:</p>

<p>To understand what <em>R__2</em> means, suppose (again) that you are trying to guess someone’s weight. If you didn’t know anything about them, your best strategy would be to guess ȳ; in that case the MSE of your guesses would be Var(<em>Y</em>):</p>

<p>But if I told you their height, you would guess <img src="figs/web/equations/img-0081.png" alt="img-0081"/> <em>x</em> <em>i</em>; in that case your MSE would be Var(<em>ε</em>).</p>

<p>So the term Var(<em>ε</em>)/Var(<em>Y</em>) is the ratio of mean squared error with and without the explanatory variable, which is the fraction of variability left unexplained by the model. The complement, <em>R__2</em>, is the fraction of variability explained by the model.</p>

<p>If a model yields <em>R__2</em> = 0.64, you could say that the model explains 64% of the variability, or it might be more precise to say that it reduces the MSE of your predictions by 64%.</p>

<p>In the context of a linear least squares model, it turns out that there is a simple relationship between the coefficient of determination and Pearson’s correlation coefficient, <em>ρ</em>:</p>

<p><em>R<em>2</em>=
<em>ρ</em>2</em></p>

<p>See <a href="http://wikipedia.org/wiki/Howzzat">http://wikipedia.org/wiki/Howzzat</a>!</p>

<p>.</p></div>

<p>.</p>
<div data-type="example">
<h5/>

<p>.</p></div>
</section>













<section data-type="sect1" id="a0000000279" data-pdf-bookmark="Correlation and Causation">
<h1>Correlation and Causation</h1>

<p>In general, a relationship between two variables does not tell you whether one causes the other, or the other way around, or both, or whether they might both be caused by something else altogether (see <a href="http://xkcd.com/552/">xkcd web comic</a>).</p>

<p>This rule can be summarized with the phrase “Correlation does not imply causation,” which is so pithy it has its own Wikipedia page: <a href="http://wikipedia.org/wiki/Correlation_does_not_imply_causation">http://wikipedia.org/wiki/Correlation_does_not_imply_causation</a>.</p>

<p>So what can you do to provide evidence of causation?</p>
<ol>
<li>
<p>Use time. If A comes before B, then A can cause B but not the other way around (at least according to our common understanding of causation). The order of events can help us infer the direction of causation, but it does not preclude the possibility that something else causes both A and B.</p>
</li>
<li>
<p>Use randomness. If you divide a large population into two groups at random and compute the means of almost any variable, you expect the difference to be small. This is a consequence of the Central Limit Theorem (so it is subject to the same requirements).</p>
</li>

</ol>

<p>If the groups are nearly identical in all variable but one, you can eliminate spurious relationships.</p>

<p>This works even if you don’t know what the relevant variables are, but it works even better if you do, because you can check that the groups are identical.</p>

<p>These ideas are the motivation for the <em>randomized controlled trial</em>, in which subjects are assigned randomly to two (or more) groups: a <em>treatment</em> group that receives some kind of intervention, like a new medicine, and a <em>control group</em> that receives no intervention, or another treatment whose effects are known.</p>

<p>A randomized controlled trial is the most reliable way to demonstrate a causal relationship, and the foundation of science-based medicine (see <a href="http://wikipedia.org/wiki/Randomized_controlled_trial">http://wikipedia.org/wiki/Randomized_controlled_trial</a>).</p>

<p>Unfortunately, controlled trials are only possible in the laboratory sciences, medicine, and a few other disciplines. In the social sciences, controlled experiments are rare, usually because they are impossible or unethical.</p>

<p>One alternative is to look for a <em>natural experiment</em>, where different “treatments” are applied to groups that are otherwise similar. One danger of natural experiments is that the groups might differ in ways that are not apparent. You can read more about this topic at <a href="http://wikipedia.org/wiki/Natural_experiment">http://wikipedia.org/wiki/Natural_experiment</a>.</p>

<p>In some cases it is possible to infer causal relationships using <em>regression analysis</em>. A linear least squares fit is a simple form of regression that explains a dependent variable using one explanatory variable. There are similar techniques that work with arbitrary numbers of independent variables.</p>

<p>I won’t cover those techniques here, but there are also simple ways to control for spurious relationships. For example, in the NSFG, we saw that first babies tend to be lighter than others (see <a data-type="xref" href="#birth_weights">???</a>). But birth weight is also correlated with the mother’s age, and mothers of first babies tend to be younger than mothers of other babies.</p>

<p>So it may be that first babies are lighter because their mothers are younger. To control for the effect of age, we could divide the mothers into age groups and compare birth weights for first babies and others in each age group.</p>

<p>If the difference between first babies and others is the same in each age group as it was in the pooled data, we conclude that the difference is not related to age. If there is no difference, we conclude that the effect is entirely due to age. Or, if the difference is smaller, we can quantify how much of the effect is due to age.</p>

<p>.</p>
<div data-type="example">
<h5/>

<p>=== Glossary</p>
<dl>
<dt>coefficient of determination</dt>
<dd>
<p>A measure of the goodness of fit of a linear model.</p>
</dd>
<dt>control group</dt>
<dd>
<p>A group in a controlled trial that receives no treatment, or a treatment whose effect is known.</p>
</dd>
<dt>correlation</dt>
<dd>
<p>a description of the dependence between variables.</p>
</dd>
<dt>covariance</dt>
<dd>
<p>a measure of the tendency of two variables to vary together.</p>
</dd>
<dt>dependent variable</dt>
<dd>
<p>A variable we are trying to predict or explain.</p>
</dd>
<dt>independent variable</dt>
<dd>
<p>A variable we are using to predict a dependent variable, also called an explanatory variable.</p>
</dd>
<dt>least squares fit</dt>
<dd>
<p>A model of a dataset that minimizes the sum of squares of the residuals.</p>
</dd>
<dt>natural experiment</dt>
<dd>
<p>An experimental design that takes advantage of a natural division of subjects into groups in ways that are at least approximately random.</p>
</dd>
<dt>normalize</dt>
<dd>
<p>To transform a set of values so that their mean is 0 and their variance is 1.</p>
</dd>
<dt>randomized controlled trial</dt>
<dd>
<p>An experimental design in which subject are divided into groups at random, and different groups are given different treatments.</p>
</dd>
<dt>rank</dt>
<dd>
<p>The index where an element appears in a sorted list.</p>
</dd>
<dt>residual</dt>
<dd>
<p>A measure of the deviation of an actual value from a model.</p>
</dd>
<dt>standard score</dt>
<dd>
<p>A value that has been normalized.</p>
</dd>
<dt>treatment</dt>
<dd>
<p>An change or intervention applied to one group in a controlled trial.</p>
</dd>
</dl></div>
</section>







<aside data-type="footnotes"><p data-type="footnote" id="idp1878768"><a href="ch08.html#idp1878768-marker"><sup>16</sup></a> See <a href="http://wikipedia.org/wiki/Cauchy-Schwarz_inequality">http://wikipedia.org/wiki/Cauchy-Schwarz_inequality</a>.</p><p data-type="footnote" id="idp1927616"><a href="ch08.html#idp1927616-marker"><sup>17</sup></a> See <a href="http://wikipedia.org/wiki/Simple_linear_regression">http://wikipedia.org/wiki/Simple_linear_regression</a>.</p><p data-type="footnote" id="idp1949712"><a href="ch08.html#idp1949712-marker"><sup>18</sup></a> See Press et al., <em>Numerical Recipes in C</em>, Chapter 15 at <a href="http://www.nrbook.com/a/bookcpdf/c15-1.pdf">http://www.nrbook.com/a/bookcpdf/c15-1.pdf</a>.</p></aside></section>
  </body>
</html>
